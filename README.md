# Awesome-LoRA
[python-img]: https://img.shields.io/github/languages/top/yuheng2000/Awesome-LoRA?color=lightgrey
[stars-img]: https://img.shields.io/github/stars/yuheng2000/Awesome-LoRA?color=yellow
[stars-url]: https://github.com/yuheng2000/Awesome-LoRA/stargazers
[fork-img]: https://img.shields.io/github/forks/yuheng2000/Awesome-LoRA?color=lightblue&label=fork
[fork-url]: https://github.com/yuheng2000/Awesome-LoRA/network/members
[visitors-img]: https://badges.pufler.dev/visits/yuheng2000/Awesome-LoRA
[adgc-url]: https://github.com/yuheng2000/Awesome-LoRA



Awesome-LoRA is a collection of state-of-the-art (SOTA), novel low-rank adaptation methods (papers, codes and datasets). Any other interesting papers and codes are welcome. Any problems, please contact jiyuheng2023@ia.ac.cn. If you find this repository useful to your research or work, it is really appreciated to star this repository. :sparkles: 

[![Made with Python][python-img]][adgc-url]
[![GitHub stars][stars-img]][stars-url]
[![GitHub forks][fork-img]][fork-url]
[![visitors][visitors-img]][adgc-url]

--------------

## What's LoRA (Low-Rank Adaptation)?

LoRA is an efficient finetuning technique proposed by Microsoft researchers to adapt large models to specific tasks and datasets.

## The pioneering paper

| Year | Title                                                        |    Venue    |                            Paper                             | Code |
| ---- | ------------------------------------------------------------ | :---------: | :----------------------------------------------------------: | :--: |
| 2021 | **LoRA: Low-Rank Adaptation of Large Language Models** |    arXiv   | [Link](https://arxiv.org/abs/2106.09685) |  [Link](https://github.com/microsoft/LoRA) |

## Important Survey Papers

| Year | Title                                                        |    Venue    |                            Paper                             | Code |
| ---- | ------------------------------------------------------------ | :---------: | :----------------------------------------------------------: | :--: |
| - | **-** |    -   | - |  - |



## Papers

| Year | Title                                                        | **Venue** |                            Paper                             |                             Code                             |
| :--: | :----------------------------------------------------------- | :-------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
| 2024 | **AdvLoRA: Adversarial Low-Rank Adaptation of Vision-Language Models** |    arXiv 2024   | [Link](https://arxiv.org/pdf/2404.13425) |  - |
| 2024 | **LoRA+: Efficient Low Rank Adaptation of Large Models** |    arXiv 2024   | [Link]() |  [Link]() |
| 2024 | **PeriodicLoRA: Breaking the Low-Rank Bottleneck in LoRA Optimization** |    arXiv 2024   | [Link]() |  [Link]() |
| 2024 | **Derivative-Free Optimization for Low-Rank Adaptation in Large Language Models** |    arXiv 2024   | [Link]() |  [Link]() |
| 2024 | **Multi-LoRA Composition for Image Generation** |    arXiv 2024   | [Link]() |  [Link]() |
| 2024 | **BiLoRA: A Bi-level Optimization Framework for Overfitting-Resilient Low-Rank Adaptation of Large Pre-trained Models** |    arXiv 2024   | [Link]() |  [Link]() |
| 2024 | **AFLoRA: Adaptive Freezing of Low Rank Adaptation in Parameter Efficient Fine-Tuning of Large Models** |    arXiv 2024   | [Link]() |  [Link]() |
| 2024 | **LoRA Meets Dropout under a Unified Framework** |    arXiv 2024   | [Link]() |  [Link]() |
| 2024 | **MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning** |    arXiv 2024   | [Link]() |  [Link]() |
| 2024 | **Galore: Memory-efficient llm training by gradient low-rank projection** |    arXiv 2024   | [Link]() |  [Link]() |
| 2024 | **Let's Focus on Neuron: Neuron-Level Supervised Fine-tuning for Large Language Model** |    arXiv 2024   | [Link]() |  [Link]() |
| 2024 | **LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning** |    arXiv 2024   | [Link]() |  [Link]() |
| 2023 | **DyLoRA: Parameter-Efficient Tuning of Pre-trained Models using Dynamic Search-Free Low-Rank Adaptation** |    EACL 2023   | [Link](https://arxiv.org/pdf/2404.13425) |  [Link](https://github.com/huawei-noah/KD-NLP/tree/main/DyLoRA) |
| 2023 | **The expressive power of low-rank adaptation** |    ICLR 2024   | [Link](https://arxiv.org/abs/2310.17513) |  [Link](https://github.com/UW-Madison-Lee-Lab/Expressive_Power_of_LoRA) |
| 2023 | **Exploring the impact of low-rank adaptation on the performance, efficiency, and regularization of RLHF** |    arXiv 2023   | [Link](https://arxiv.org/abs/2309.09055) |  [Link](https://github.com/SimengSun/alpaca_farm_lora) |
| 2023 | **Deep Learning Model Compression With Rank Reduction in Tensor Decomposition** |    TNNLS 2023   | [Link]() |  [Link]() |
| 2023 | **Loramoe: Revolutionizing mixture of experts for maintaining world knowledge in language model alignment** |    arXiv 2023   | [Link]() |  [Link]() |
| 2023 | **Bayesian Low-rank Adaptation for Large Language Models** |    ICLR 2024   | [Link](https://arxiv.org/abs/2308.13111) |  [Link](https://github.com/MaximeRobeyns/bayesian_lora) |
| 2023 | **Lora-fa: Memory-efficient low-rank adaptation for large language models fine-tuning** |    arXiv 2023   | [Link](https://arxiv.org/abs/2308.03303) |  - |
| 2021 | **LoRA: Low-Rank Adaptation of Large Language Models** |    arXiv 2021   | [Link](https://arxiv.org/abs/2106.09685) |  [Link](https://github.com/microsoft/LoRA) |





